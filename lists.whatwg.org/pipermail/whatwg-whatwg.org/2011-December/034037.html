<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [whatwg] PeerConnection feedback
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20PeerConnection%20feedback&In-Reply-To=%3CPine.LNX.4.64.1112022344230.9078%40ps20323.dreamhostps.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="076309.html">
   <LINK REL="Next"  HREF="034038.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[whatwg] PeerConnection feedback</H1>
<!--htdig_noindex-->
    <B>Ian Hickson</B> 
    <A HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20PeerConnection%20feedback&In-Reply-To=%3CPine.LNX.4.64.1112022344230.9078%40ps20323.dreamhostps.com%3E"
       TITLE="[whatwg] PeerConnection feedback">ian at hixie.ch
       </A><BR>
    <I>Fri Dec  2 16:00:50 PST 2011</I>
    <P><UL>
        <LI>Previous message: <A HREF="076309.html">[whatwg] Behavior when &lt;script&gt; is removed from DOM
</A></li>
        <LI>Next message: <A HREF="034038.html">[whatwg] Stat. on frequency of node insertion without children
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#34037">[ date ]</a>
              <a href="thread.html#34037">[ thread ]</a>
              <a href="subject.html#34037">[ subject ]</a>
              <a href="author.html#34037">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--/htdig_noindex-->
<!--beginarticle-->
<PRE>
I include below, for posterity, some feedback to which I will not be 
replying, as it relates to the PeerConnection and media streams section of 
the specification which has since been moved to the WebRTC working group 
at the W3C.

I encourage anyone who is interested in that particular topic to follow 
the aforementioned group.

On Tue, 26 Jul 2011, Mark Callow wrote:
&gt;<i> On 26/07/2011 14:30, Ian Hickson wrote:
</I>&gt;<i> &gt; On Thu, 14 Jul 2011 04:09:40 +0530, Ian Hickson &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">ian at hixie.ch</A>&gt; wrote:
</I>&gt;<i> &gt; &gt; &gt; &gt; &gt; 
</I>&gt;<i> &gt; &gt; &gt; &gt; &gt; Another question is flash. As far as I have seen, there seems 
</I>&gt;<i> &gt; &gt; &gt; &gt; &gt; to be no option to specify whether the camera needs to use 
</I>&gt;<i> &gt; &gt; &gt; &gt; &gt; flash or not. Is this decision left up to the device? (If 
</I>&gt;<i> &gt; &gt; &gt; &gt; &gt; someone is making an app which is just clicking a picture of 
</I>&gt;<i> &gt; &gt; &gt; &gt; &gt; the person, then it would be nice to have the camera use flash 
</I>&gt;<i> &gt; &gt; &gt; &gt; &gt; in low light conditions).
</I>&gt;<i> &gt; &gt; &gt; &gt;
</I>&gt;<i> &gt; &gt; &gt; &gt; getUserMedia() returns a video stream, so it wouldn't use a 
</I>&gt;<i> &gt; &gt; &gt; &gt; flash.
</I>&gt;<i> &gt; &gt; 
</I>&gt;<i> &gt; &gt; Wouldn't it make sense to have a provision for flash separately 
</I>&gt;<i> &gt; &gt; then? I think a lot of apps would like just a picture instead of 
</I>&gt;<i> &gt; &gt; video, and in those cases, flash would be required. Maybe a seperate 
</I>&gt;<i> &gt; &gt; provision in the spec which defines whether to use flash, and if so, 
</I>&gt;<i> &gt; &gt; for how many miliseconds. Is that doable?
</I>&gt;<i>
</I>&gt;<i> There is a lot more that could be done than simply triggering the flash. 
</I>&gt;<i> See /The Frankencamera: An Experimental Platform for Computational 
</I>&gt;<i> Photography/ &lt;<A HREF="http://graphics.stanford.edu/papers/fcam/">http://graphics.stanford.edu/papers/fcam/</A>&gt; and The FCAM 
</I>&gt;<i> API &lt;<A HREF="http://fcam.garage.maemo.org/">http://fcam.garage.maemo.org/</A>&gt;.
</I>
On Tue, 26 Jul 2011, Tommy Widenflycht (&#225;~[~O&#225;~Z&#174;&#225;~[~X&#225;~[~X&#225;~Z&#164;) wrote:
&gt;<i> On Tue, Jul 26, 2011 at 07:30, Ian Hickson &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">ian at hixie.ch</A>&gt; wrote:
</I>&gt;<i> &gt; &gt;
</I>&gt;<i> &gt; &gt; If you send two MediaStream objects constructed from the same 
</I>&gt;<i> &gt; &gt; LocalMediaStream over a PeerConnection there needs to be a way to 
</I>&gt;<i> &gt; &gt; separate them on the receiving side.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; What's the use case for sending the same feed twice?
</I>&gt;<i> 
</I>&gt;<i> There's no proper use case as such but the spec allows this.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; &gt; I also think it is a bit unfortunate that we now have a 'label' 
</I>&gt;<i> &gt; &gt; property on the track objects that means something else than the 
</I>&gt;<i> &gt; &gt; 'label' property on MediaStream, perhaps 'description' would be a 
</I>&gt;<i> &gt; &gt; more suitable name for the former.
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; In what sense do they mean different things? I don't understand the 
</I>&gt;<i> &gt; problem here. Can you elaborate?
</I>&gt;<i> 
</I>&gt;<i> label on a MediaStream is a unique identifier, while the label on a 
</I>&gt;<i> MediaStreamTrack is just a description like &quot;Logitech Vision Pro&quot;, &quot;Line 
</I>&gt;<i> In&quot; or &quot;Built-in Mic&quot;. I too find this a bit odd.
</I>&gt;<i>
</I>&gt;<i> [...]
</I>&gt;<i>
</I>&gt;<i> If I may make an analogy to the real world: plumbing.
</I>&gt;<i> 
</I>&gt;<i> Each fork of a MediaStream is a new joint in the pipe, my suggestion 
</I>&gt;<i> introduces a tap at each joint. No matter how you open and close the tap 
</I>&gt;<i> at the end (or middle); if any previous tap is closed there's nothing 
</I>&gt;<i> coming through. The spec currently removes and add the entire pipe after 
</I>&gt;<i> the changed joint.
</I>&gt;<i>
</I>&gt;<i> &gt; &gt; Also some follow-up questions regarding the new TrackLists:
</I>&gt;<i> &gt; &gt;
</I>&gt;<i> &gt; &gt; What should happen when a track fails? Should the entire stream 
</I>&gt;<i> &gt; &gt; fail, the MSTrack silently be removed or the MSTrack disassociated 
</I>&gt;<i> &gt; &gt; with the track (and thus becoming a do-nothing object)?
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; What do you mean by &quot;fails&quot;?
</I>&gt;<i> 
</I>&gt;<i> Yanking the USB cable to the camera for example. This should imho stop 
</I>&gt;<i> the MS, not just silently send black video.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; What should happen when a stream with two or more video tracks is 
</I>&gt;<i> &gt; &gt; associated to a &lt;video&gt; tag? Just render the first enabled one?
</I>&gt;<i> &gt;
</I>&gt;<i> &gt; Same as if you had a regular video file with multiple tracks.
</I>&gt;<i>
</I>&gt;<i> And that is? Sorry, this might be written down somewhere and I have 
</I>&gt;<i> missed it.
</I>
On Thu, 28 Jul 2011, Stefan H&#229;kansson LK wrote:
&gt;<i> &gt;On Tue, Jul 26, 2011 at 07:30, Ian Hickson &lt;ian at hixie.ch&gt; wrote:
</I>&gt;<i> &gt;&gt;
</I>&gt;<i> &gt;&gt; &gt; If you send two MediaStream objects constructed from the same 
</I>&gt;<i> &gt;&gt; &gt; LocalMediaStream over a PeerConnection there needs to be a way to 
</I>&gt;<i> &gt;&gt; &gt; separate them on the receiving side.
</I>&gt;<i> &gt;&gt;
</I>&gt;<i> &gt;&gt; What's the use case for sending the same feed twice?
</I>&gt;<i> &gt;
</I>&gt;<i> &gt;There's no proper use case as such but the spec allows this.
</I>&gt;<i>
</I>&gt;<i> The question is how serious a problem this is. If you want to fork, and 
</I>&gt;<i> make both (all) versions available at the peer, would you not transmit 
</I>&gt;<i> the full stream and fork at the receiving end for efficiency reasons? 
</I>&gt;<i> And if you really want to fork at the sender, one way to separate them 
</I>&gt;<i> is to use one PeerConnection per fork.
</I>
On Tue, 2 Aug 2011, Per-Erik Brodin wrote:
&gt;<i> On 2011-07-26 07:30, Ian Hickson wrote:
</I>&gt;<i> &gt; On Tue, 19 Jul 2011, Per-Erik Brodin wrote:
</I>&gt;<i> &gt; &gt; 
</I>&gt;<i> &gt; &gt; Perhaps now that there is no longer any relation to tracks on the 
</I>&gt;<i> &gt; &gt; media elements we could also change Track to something else, maybe 
</I>&gt;<i> &gt; &gt; Component. I have had people complaining to me that Track is not 
</I>&gt;<i> &gt; &gt; really a good name here.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; I'm happy to change the name if there's a better one. I'm not sure 
</I>&gt;<i> &gt; Component is any better than Track though.
</I>&gt;<i> 
</I>&gt;<i> OK, let's keep Track until someone comes up with a better name then.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; Good. Could we still keep audio and video in separate lists though? 
</I>&gt;<i> &gt; &gt; It makes it easier to check the number of audio or video components 
</I>&gt;<i> &gt; &gt; and you can avoid loops that have to check the kind for each 
</I>&gt;<i> &gt; &gt; iteration if you only want to operate on one media type.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; Well in most (almost all?) cases, there'll be at most one audio track 
</I>&gt;<i> &gt; and at most one video track, which is why I didn't put them in 
</I>&gt;<i> &gt; separate lists. What use cases did you have in mind where there would 
</I>&gt;<i> &gt; be enough tracks that it would be better for them to be separate 
</I>&gt;<i> &gt; lists?
</I>&gt;<i> 
</I>&gt;<i> Yes, you're right, but even with zero or one track it's more convenient 
</I>&gt;<i> to have them separate because that way you can more easily check if the 
</I>&gt;<i> stream contains any audio and/or video tracks and check the number of 
</I>&gt;<i> tracks of each kind. I also think it will be problematic if we would 
</I>&gt;<i> like to add another kind at a later stage if all tracks are in the same 
</I>&gt;<i> list since people will make assumptions that audio and video are the 
</I>&gt;<i> only kinds.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; I also think that it would be easier to construct new MediaStream 
</I>&gt;<i> &gt; &gt; objects from individual components rather than temporarily disabling 
</I>&gt;<i> &gt; &gt; the ones you do not want to copy to the new MediaStream object and 
</I>&gt;<i> &gt; &gt; then re-enabling them again afterwards.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; Re-enabling them afterwards would re-include them in the copies, too.
</I>&gt;<i> 
</I>&gt;<i> Why is this needed? If a new MediaStream object is constructed from 
</I>&gt;<i> another MediaStream I think it would be simpler to just let that be a 
</I>&gt;<i> clone of the stream with all tracks present (with the enabled/disabled 
</I>&gt;<i> states independently set).
</I>&gt;<i> 
</I>&gt;<i> &gt; The main use case here is temporarily disabling a video or audio track 
</I>&gt;<i> &gt; in a video conference. I don't understand how your proposal would work 
</I>&gt;<i> &gt; for that. Can you elaborate?
</I>&gt;<i> 
</I>&gt;<i> A new MediaStream object is created from the video track of a 
</I>&gt;<i> LocalMediaStream to be used as self-view. The LocalMediaStream can then 
</I>&gt;<i> be sent over PeerConnection and the video track disabled without 
</I>&gt;<i> affecting the MediaStream being played back locally in the self-view. In 
</I>&gt;<i> addition, my proposal opens up for additional use cases that require 
</I>&gt;<i> combining tracks from different streams, such as recording a 
</I>&gt;<i> conversation (a number of audio tracks from various streams, local and 
</I>&gt;<i> remote combined to a single stream).
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; It is also unclear to me what happens to a LocalMediaStream object 
</I>&gt;<i> &gt; &gt; that is currently being consumed in that case.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; Not sure what you mean. Can you elaborate?
</I>&gt;<i> 
</I>&gt;<i> I was under the impression that, if a stream of audio and video is being 
</I>&gt;<i> sent to one peer and then another peer joins but only audio should be 
</I>&gt;<i> sent, then video would have to be temporarily disabled in the first 
</I>&gt;<i> stream in order to construct a new MediaStream object containing only 
</I>&gt;<i> the audio track. Again, it would be simpler to construct a new 
</I>&gt;<i> MediaStream object from just the audio track and send that.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; Why should the label the same as the parent on the newly constructed 
</I>&gt;<i> &gt; &gt; MediaStream object?
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; The label identifies the source of the media. It's the same source, 
</I>&gt;<i> &gt; so, same label.
</I>&gt;<i> 
</I>&gt;<i> I agree, but usually you have more than one source in a MediaStream and 
</I>&gt;<i> if you construct a new MediaStream from it which doesn't contain all of 
</I>&gt;<i> the sources from the parent I don't think the label should be the same. 
</I>&gt;<i> By the way, what happens if you call getUserMedia() twice and get the 
</I>&gt;<i> same set of sources both times, do you get the same label then? What if 
</I>&gt;<i> the user selects different sources the second time?
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; If you send two MediaStream objects constructed from the same 
</I>&gt;<i> &gt; &gt; LocalMediaStream over a PeerConnection there needs to be a way to 
</I>&gt;<i> &gt; &gt; separate them on the receiving side.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; What's the use case for sending the same feed twice?
</I>&gt;<i> 
</I>&gt;<i> If the labels are the same then that should indicate that it's 
</I>&gt;<i> essentially the same stream and there should be no need to send it 
</I>&gt;<i> twice. If the streams are not composed of the same underlying sources 
</I>&gt;<i> then you may want to send them both and the labels should differ.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; I also think it is a bit unfortunate that we now have a 'label' 
</I>&gt;<i> &gt; &gt; property on the track objects that means something else than the 
</I>&gt;<i> &gt; &gt; 'label' property on MediaStream, perhaps 'description' would be a 
</I>&gt;<i> &gt; &gt; more suitable name for the former.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; In what sense do they mean different things? I don't understand the 
</I>&gt;<i> &gt; problem here. Can you elaborate?
</I>&gt;<i> 
</I>&gt;<i> As Tommy pointed out, label on MediaStream is an identifier for the 
</I>&gt;<i> stream whereas label och MediaStreamTrack is a description of the 
</I>&gt;<i> source.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; &gt; The current design is just the result of needing to define what 
</I>&gt;<i> &gt; &gt; &gt; happens when you call getRecordedData() twice in a row. Could you 
</I>&gt;<i> &gt; &gt; &gt; elaborate on what API you think we should have?
</I>&gt;<i> &gt; &gt; 
</I>&gt;<i> &gt; &gt; What I am thinking of is something similar to what was proposed in 
</I>&gt;<i> &gt; &gt; <A HREF="http://lists.whatwg.org/htdig.cgi/whatwg-whatwg.org/2011-March/030921.html">http://lists.whatwg.org/htdig.cgi/whatwg-whatwg.org/2011-March/030921.html</A>
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; That doesn't answer the question of what happens if you call stop() 
</I>&gt;<i> &gt; twice.
</I>&gt;<i> 
</I>&gt;<i> Nothing will happen the second time since recording has already stopped.
</I>&gt;<i> 
</I>&gt;<i> &gt; (Also, having to call a method and hook an event so that you can read 
</I>&gt;<i> &gt; an attribute seems like a rather round-about way of getting data. Is 
</I>&gt;<i> &gt; calling a method with a callback not simpler?)
</I>&gt;<i> 
</I>&gt;<i> When the event has been fired you can read the attribute whenever you 
</I>&gt;<i> want to get the blob, how many times you want. I prefer that over having 
</I>&gt;<i> stop() take a callback argument.
</I>&gt;<i> 
</I>&gt;<i> &gt; Quota doesn't seem particularly important here. It's not like you can 
</I>&gt;<i> &gt; really do lasting damage. It would just be a DOS attack, like creating 
</I>&gt;<i> &gt; a Web page with an infinite number of 10000x10000 canvases. We can 
</I>&gt;<i> &gt; just let the &quot;hardware limitation&quot; clause handle it.
</I>&gt;<i> 
</I>&gt;<i> In a video blog recording application it would be nice to be able to 
</I>&gt;<i> present to the user how much more can be recorded and not just handle it 
</I>&gt;<i> as a hardware limitation, since that could mean dropping the entire 
</I>&gt;<i> recording.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; I was not saying that it would not be possible to keep track of 
</I>&gt;<i> &gt; &gt; which blob: URLs that point to blobs and which point to streams just 
</I>&gt;<i> &gt; &gt; that we want to avoid doing that in the early stage of the media 
</I>&gt;<i> &gt; &gt; engine selection. In my opinion a stream is quite the opposite of a 
</I>&gt;<i> &gt; &gt; blob (unknown, perhaps infinite length vs. fixed length) so when 
</I>&gt;<i> &gt; &gt; printing the URLs for debugging purposes it would also be much nicer 
</I>&gt;<i> &gt; &gt; to have two different protocol schemes. If I remember correctly the 
</I>&gt;<i> &gt; &gt; discussions leading up to the renaming of createBlobURL to 
</I>&gt;<i> &gt; &gt; createObjectURL assumed that there would be stream: URLs.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; You wouldn't be able to remove that logic, since http: URLs would 
</I>&gt;<i> &gt; still have the same needs. You can have finite and infinite http: 
</I>&gt;<i> &gt; resources, just like you can have finite and infinite blob: resources. 
</I>&gt;<i> &gt; I don't really see the problem here. Indeed, with blob:, it's trivial 
</I>&gt;<i> &gt; to find out if the resource is finite or not; with http: you might not 
</I>&gt;<i> &gt; know until the whole finite resource is downloaded.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; If there is something I'm missing here please do let me know.
</I>&gt;<i> 
</I>&gt;<i> The differentiation is not between finite and infinite resources but 
</I>&gt;<i> rather between playback media resources and conversational media 
</I>&gt;<i> resources. blob: and http: are both handled by the playback media engine 
</I>&gt;<i> whereas stream: is handled by the conversational media engine. We would 
</I>&gt;<i> like to be able to determine which engine to use by simply looking at 
</I>&gt;<i> the URL.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; &gt; &gt; PeerConnection is an EventTarget but it still uses a callback 
</I>&gt;<i> &gt; &gt; &gt; &gt; for the signaling messages and this mixture of events and 
</I>&gt;<i> &gt; &gt; &gt; &gt; callbacks is a bit awkward in my opinion. If you would like to 
</I>&gt;<i> &gt; &gt; &gt; &gt; change the function that handles signaling messages after 
</I>&gt;<i> &gt; &gt; &gt; &gt; calling the constructor you would have to wrap a function call 
</I>&gt;<i> &gt; &gt; &gt; &gt; inside the callback to the actual signal handling function, 
</I>&gt;<i> &gt; &gt; &gt; &gt; instead of just (re-)setting an onsignal (or whatever) attribute 
</I>&gt;<i> &gt; &gt; &gt; &gt; listener (the event could reuse the MessageEvent interface).
</I>&gt;<i> &gt; &gt; &gt; 
</I>&gt;<i> &gt; &gt; &gt; When would you change the callback?
</I>&gt;<i> &gt; &gt; 
</I>&gt;<i> &gt; &gt; If you would like to send the signaling messages peer-to-peer over 
</I>&gt;<i> &gt; &gt; the data channel, once it is established.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; That seems like a disaster waiting to happen. The UDP data channel is 
</I>&gt;<i> &gt; unreliable, the signaling channel has to be reliable. Worse, the UDP 
</I>&gt;<i> &gt; data channel might go down at any second, and then the user agent 
</I>&gt;<i> &gt; would try to re-establish it using the signaling channel.
</I>&gt;<i> 
</I>&gt;<i> You can provide a reliable channel on top of the unreliable channel and 
</I>&gt;<i> monitor the PeerConnection state so that you know when to fall back to 
</I>&gt;<i> server-relayed signaling. One reason to do this would be to improve the 
</I>&gt;<i> signaling latency which can be of importance in applications that, for 
</I>&gt;<i> example, trigger format renegotiation due to change in video display 
</I>&gt;<i> size.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; &gt; - It's easy to not register a callback, which makes no sense. 
</I>&gt;<i> &gt; &gt; &gt; There's literally never a use for create a PeerConnection without 
</I>&gt;<i> &gt; &gt; &gt; a signaling channel, as far as I can tell, so making it easier to 
</I>&gt;<i> &gt; &gt; &gt; create one without a callback than with seems like a bad design.
</I>&gt;<i> &gt; &gt; 
</I>&gt;<i> &gt; &gt; For example, creating an EventSource without registering any 
</I>&gt;<i> &gt; &gt; listener for incoming events equally does not make sense.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; Actually, it does. One operation mode for EventSource is to have 
</I>&gt;<i> &gt; events with different names, each triggering a different event 
</I>&gt;<i> &gt; listener.
</I>&gt;<i> 
</I>&gt;<i> An EventSource without any event listener seems rather useless to me. 
</I>&gt;<i> Even if you can assign multiple handlers for events with different 
</I>&gt;<i> names, all those handlers could still be provided as arguments to the 
</I>&gt;<i> constructor, right? That would ensure that nobody can create an 
</I>&gt;<i> EventSource without registering at least one event listener.
</I>&gt;<i> 
</I>&gt;<i> &gt; &gt; &gt; &gt; There is a potential problem in the exchange of SDPs in that 
</I>&gt;<i> &gt; &gt; &gt; &gt; glare conditions can occur if both peers add streams 
</I>&gt;<i> &gt; &gt; &gt; &gt; simultaneously, in which case there will be two different 
</I>&gt;<i> &gt; &gt; &gt; &gt; outstanding offers that none of the peers are allowed to respond 
</I>&gt;<i> &gt; &gt; &gt; &gt; to according to the SDP offer-answer model. Instead of using one 
</I>&gt;<i> &gt; &gt; &gt; &gt; SDP session for all media as the specification suggests, we are 
</I>&gt;<i> &gt; &gt; &gt; &gt; handling the offer-answer for each stream separately to avoid 
</I>&gt;<i> &gt; &gt; &gt; &gt; such conditions.
</I>&gt;<i> &gt; &gt; &gt; 
</I>&gt;<i> &gt; &gt; &gt; Why isn't this handled by the ICE role conflict processing rules? 
</I>&gt;<i> &gt; &gt; &gt; It seems like simultaneous ICE restarts would be trivially 
</I>&gt;<i> &gt; &gt; &gt; resolvable by just following the rules in the ICE spec. Am I 
</I>&gt;<i> &gt; &gt; &gt; missing something?
</I>&gt;<i> &gt; &gt; 
</I>&gt;<i> &gt; &gt; This problem is not related to ICE but rather to the SDP 
</I>&gt;<i> &gt; &gt; offer-answer model which is separate from the ICE processing. The 
</I>&gt;<i> &gt; &gt; problem is that SDP offer-answer does not allow you to respond to an 
</I>&gt;<i> &gt; &gt; offer when you have an outstanding offer for the same set of 
</I>&gt;<i> &gt; &gt; streams.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; As far as I can tell, your interpretation is incorrect. This is 
</I>&gt;<i> &gt; entirely related to ICE, and ICE, as far as I can tell, defines this 
</I>&gt;<i> &gt; exact case in its role conflict resolution.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; The only time this can happen is if you have both ends do an ICE 
</I>&gt;<i> &gt; restart at exactly the same time. The offer from each ICE agent will 
</I>&gt;<i> &gt; be received by the other as if it was the response, and thus there 
</I>&gt;<i> &gt; will be a role conflict and the ICE role conflict resolution process 
</I>&gt;<i> &gt; will kick in. No?
</I>&gt;<i> 
</I>&gt;<i> No, an ICE role conflict is not the same thing as a glare condition in 
</I>&gt;<i> SDP offer-answer.
</I>
On Wed, 27 Jul 2011, Rob Manson wrote:
&gt;<i> 
</I>&gt;<i> This is definitely not intended as criticism of any of the work going 
</I>&gt;<i> on.  It's intended as constructive feedback that hopefully provides 
</I>&gt;<i> clarification on a key use case and it's supporting requirements.
</I>&gt;<i> 
</I>&gt;<i>         &quot;Access to live/raw audio and video stream data from both local
</I>&gt;<i>         and remote sources in a consistent way&quot;
</I>&gt;<i> 
</I>&gt;<i> I've spent quite a bit of time trying to follow a clear thread of 
</I>&gt;<i> requirements/solutions that provide API access to raw stream data (e.g. 
</I>&gt;<i> audio, video, etc.).  But I'm a bit concerned this is falling in the gap 
</I>&gt;<i> between the DAP and RTC WGs.  If this is not the case then please point 
</I>&gt;<i> me to the relevant docs and I'll happily get back in my box 8)
</I>&gt;<i> 
</I>&gt;<i> Here's how the thread seems to flow at the moment based on public 
</I>&gt;<i> documents.
</I>&gt;<i> 
</I>&gt;<i> On the DAP page [1] the mission states:
</I>&gt;<i>         &quot;the Device APIs and Policy Working Group is to create
</I>&gt;<i>         client-side APIs that enable the development of Web Applications
</I>&gt;<i>         and Web Widgets that interact with devices services such as
</I>&gt;<i>         Calendar, Contacts, Camera, etc&quot;
</I>&gt;<i> 
</I>&gt;<i> So it seems clear that this is the place to start.  Further down that 
</I>&gt;<i> page the &quot;HTML Media Capture&quot; and &quot;Media Capture&quot; APIs are listed.
</I>&gt;<i> 
</I>&gt;<i> HTML Media Capture (camera/microphone interactions through HTML forms) 
</I>&gt;<i> initially seems like a good candidate, however the intro in the latest 
</I>&gt;<i> PWD [2] clearly states:
</I>&gt;<i>         &quot;Providing streaming access to these capabilities is outside of
</I>&gt;<i>         the scope of this specification.&quot;
</I>&gt;<i> 
</I>&gt;<i> Followed by a NOTE that states:
</I>&gt;<i>         &quot;The Working Group is investigating the opportunity to specify
</I>&gt;<i>         streaming access via the proposed &lt;device&gt; element.&quot;
</I>&gt;<i>         The link on the &quot;proposed &lt;device&gt; element&quot; [3] links to a &quot;no 
</I>&gt;<i> longer maintained&quot; document that then redirects to the top level of the 
</I>&gt;<i> whatwg &quot;current work&quot; page [4].  On that page the most relevant link is 
</I>&gt;<i> the video conferencing and peer-to-peer communication section [5].  
</I>&gt;<i> More about that further below.
</I>&gt;<i> 
</I>&gt;<i> So back to the DAP page to follow explore the other Media Capture API 
</I>&gt;<i> (programmatic access to camera/microphone) [1] and it's latest PWD [6].
</I>&gt;<i>
</I>&gt;<i> The abstract states:
</I>&gt;<i>
</I>&gt;<i>         &quot;This specification defines an Application Programming Interface
</I>&gt;<i>         (API) that provides access to the audio, image and video capture
</I>&gt;<i>         capabilities of the device.&quot;
</I>&gt;<i> 
</I>&gt;<i> And the introduction states:
</I>&gt;<i>
</I>&gt;<i>         &quot;The Capture API defines a high-level interface for accessing
</I>&gt;<i>         the microphone and camera of a hosting device. It completes the
</I>&gt;<i>         HTML Form Based Media Capturing specification [HTMLMEDIACAPTURE]
</I>&gt;<i>         with a programmatic access to start a parametrized capture
</I>&gt;<i>         process.&quot;
</I>&gt;<i>
</I>&gt;<i> So it seems clear that this is not related to streams in any way either.
</I>&gt;<i> 
</I>&gt;<i> The Notes column for this API on the DAP page [1] also states:
</I>&gt;<i>         &quot;Programmatic API that completes the form based approach
</I>&gt;<i>         Need to check if still interest in this
</I>&gt;<i>         How does it relate with the Web RTC Working Group?&quot;
</I>&gt;<i> 
</I>&gt;<i> Is there an updated position on this?
</I>&gt;<i> 
</I>&gt;<i> So if you then head over to the WebRTC WG's charter [7] it states:
</I>&gt;<i>         &quot;...to define client-side APIs to enable Real-Time
</I>&gt;<i>         Communications in Web browsers.
</I>&gt;<i>         
</I>&gt;<i>         These APIs should enable building applications that can be run
</I>&gt;<i>         inside a browser, requiring no extra downloads or plugins, that
</I>&gt;<i>         allow communication between parties using audio, video and
</I>&gt;<i>         supplementary real-time communication, without having to use
</I>&gt;<i>         intervening servers...&quot;
</I>&gt;<i>         So this is clearly focused upon peer-to-peer communication 
</I>&gt;<i> &quot;between&quot; systems and the stream related access is naturally just 
</I>&gt;<i> treated as an ancillary requirement.  The scope section then states:
</I>&gt;<i>         &quot;Enabling real-time communications between Web browsers require
</I>&gt;<i>         the following client-side technologies to be available:
</I>&gt;<i>         
</I>&gt;<i>         - API functions to explore device capabilities, e.g. camera,
</I>&gt;<i>         microphone, speakers (currently in scope for the Device APIs &amp;
</I>&gt;<i>         Policy Working Group)
</I>&gt;<i>         - API functions to capture media from local devices (camera and
</I>&gt;<i>         microphone) (currently in scope for the Device APIs &amp; Policy
</I>&gt;<i>         Working Group)
</I>&gt;<i>         - API functions for encoding and other processing of those media
</I>&gt;<i>         streams,
</I>&gt;<i>         - API functions for establishing direct peer-to-peer
</I>&gt;<i>         connections, including firewall/NAT traversal
</I>&gt;<i>         - API functions for decoding and processing (including echo
</I>&gt;<i>         cancelling, stream synchronization and a number of other
</I>&gt;<i>         functions) of those streams at the incoming end,
</I>&gt;<i>         - Delivery to the user of those media streams via local screens
</I>&gt;<i>         and audio output devices (partially covered with HTML5)&quot;
</I>&gt;<i>         
</I>&gt;<i> So this is where I really start to feel the gap growing.  The DAP is
</I>&gt;<i> pointing to RTC saying not sure how if our Camera/Microphone APIs are
</I>&gt;<i> being superseded by the work in the RTC...and the RTC then points back
</I>&gt;<i> to say it will be relying on work in the DAP.  However the RTCs
</I>&gt;<i> Recommended Track Deliverables list does include:
</I>&gt;<i>         &quot;Media Stream Functions, Audio Stream Functions and Video Stream
</I>&gt;<i>         Functions&quot;
</I>&gt;<i> 
</I>&gt;<i> So then it's back to the whatwg MediaStream and LocalMediaStream current
</I>&gt;<i> work [8].  Following this through you end up back at the &lt;audio&gt; and
</I>&gt;<i> &lt;video&gt; media element with some brief discussion about media data [9].
</I>&gt;<i> 
</I>&gt;<i> Currently the only API that I'm aware of that allows live access to the
</I>&gt;<i> audio data through the &lt;audio&gt; tag is the relatively proprietary Mozilla
</I>&gt;<i> Audio Data API [10].
</I>&gt;<i> 
</I>&gt;<i> And while the video stream data can be accessed by rendering each frame
</I>&gt;<i> into a canvas 2d graphics context and then using getImageData to extract
</I>&gt;<i> and manipulate it from there [11], this seems more like a work around
</I>&gt;<i> than an elegantly designed solution.
</I>&gt;<i>  
</I>&gt;<i> As I said above, this is not intended as a criticism of the work that
</I>&gt;<i> the DAP WG, WebRTC WG or WHATWG are doing.  It's intended as
</I>&gt;<i> constructive feedback to highlight that the important use case of
</I>&gt;<i> &quot;Access to live/raw audio and video stream data from both local and
</I>&gt;<i> remote sources&quot; appears to be falling in the gaps between the groups. 
</I>&gt;<i> 
</I>&gt;<i> From my perspective this is a critical use case for many advanced web
</I>&gt;<i> apps that will help bring them in line with what's possible in the
</I>&gt;<i> native single vendor stack based apps at the moment (e.g. iPhone &amp;
</I>&gt;<i> Android).  And it's also critical for the advancement of web standards
</I>&gt;<i> based AR applications and other computer vision, hearing and signal
</I>&gt;<i> processing applications.
</I>&gt;<i> 
</I>&gt;<i> I understand that a lot of these specifications I've covered are in very
</I>&gt;<i> formative stages and that requirements and PWDs are just being drafted
</I>&gt;<i> as I write.  And that's exactly why I'm raising this as a single and
</I>&gt;<i> consolidated perspective that spans all these groups.  I hope this goes
</I>&gt;<i> some way towards &quot;Access to live/raw audio and video stream data from
</I>&gt;<i> both local and remote sources&quot; being treated as an essential and core
</I>&gt;<i> use case that binds together the work of all these groups.  With a clear
</I>&gt;<i> vision for this and a little consolidated work I think this will then
</I>&gt;<i> also open up a wide range of other app opportunities that we haven't
</I>&gt;<i> even thought of yet.  But at the moment it really feels like this is
</I>&gt;<i> being treated as an assumed requirement and could end up as a poorly
</I>&gt;<i> formed second class bundle of semi-related API hooks.
</I>&gt;<i> 
</I>&gt;<i> For this use case I'd really like these clear requirements to be
</I>&gt;<i> supported:
</I>&gt;<i> - access the raw stream data for both audio and video in similar ways
</I>&gt;<i> - access the raw stream data from both remote and local streams in
</I>&gt;<i> similar ways
</I>&gt;<i> - ability to inject new data or the transformed original data back into
</I>&gt;<i> streams and presented audio/video tags in a consistent way
</I>&gt;<i> - all of this be optimised for performance to meet the demands of live
</I>&gt;<i> signal processing
</I>&gt;<i> 
</I>&gt;<i> PS: I've also cc'd in the mozilla dev list as I think this directly
</I>&gt;<i> relates to the current &quot;booting to the web&quot; thread [12]
</I>&gt;<i> 
</I>&gt;<i> [1] <A HREF="http://www.w3.org/2009/dap/">http://www.w3.org/2009/dap/</A>
</I>&gt;<i> [2] <A HREF="http://www.w3.org/TR/2011/WD-html-media-capture-20110414/#introduction">http://www.w3.org/TR/2011/WD-html-media-capture-20110414/#introduction</A>
</I>&gt;<i> [3] <A HREF="http://dev.w3.org/html5/html-device/">http://dev.w3.org/html5/html-device/</A> 
</I>&gt;<i> [4] <A HREF="http://www.whatwg.org/specs/web-apps/current-work/complete/#devices">http://www.whatwg.org/specs/web-apps/current-work/complete/#devices</A> 
</I>&gt;<i> [5] <A HREF="http://www.whatwg.org/specs/web-apps/current-work/complete/#auto-toc-9">http://www.whatwg.org/specs/web-apps/current-work/complete/#auto-toc-9</A>
</I>&gt;<i> [6] <A HREF="http://www.w3.org/TR/2010/WD-media-capture-api-20100928/">http://www.w3.org/TR/2010/WD-media-capture-api-20100928/</A>
</I>&gt;<i> [7] <A HREF="http://www.w3.org/2011/04/webrtc-charter.html">http://www.w3.org/2011/04/webrtc-charter.html</A>
</I>&gt;<i> [8] <A HREF="http://www.whatwg.org/specs/web-apps/current-work/complete/video-conferencing-and-peer-to-peer-communication.html#mediastream">http://www.whatwg.org/specs/web-apps/current-work/complete/video-conferencing-and-peer-to-peer-communication.html#mediastream</A> 
</I>&gt;<i> [9] <A HREF="http://www.whatwg.org/specs/web-apps/current-work/complete/the-iframe-element.html#media-data">http://www.whatwg.org/specs/web-apps/current-work/complete/the-iframe-element.html#media-data</A>
</I>&gt;<i> [10] <A HREF="https://wiki.mozilla.org/Audio_Data_API">https://wiki.mozilla.org/Audio_Data_API</A>
</I>&gt;<i> [11] <A HREF="https://developer.mozilla.org/En/Manipulating_video_using_canvas">https://developer.mozilla.org/En/Manipulating_video_using_canvas</A>
</I>&gt;<i> [12] <A HREF="http://groups.google.com/group/mozilla.dev.platform/browse_thread/thread/7668a9d46a43e482#">http://groups.google.com/group/mozilla.dev.platform/browse_thread/thread/7668a9d46a43e482#</A> 
</I>
On Fri, 12 Aug 2011, Darin Fisher wrote:
&gt;<i>
</I>&gt;<i> Putting implementation details aside, I agree that it is a bit 
</I>&gt;<i> unfortunate to refer to a stream as a blob.  So far, blobs have always 
</I>&gt;<i> referred to static, fixed-size things.
</I>&gt;<i> 
</I>&gt;<i> This function was originally named createBlobURL, but it was renamed 
</I>&gt;<i> createObjectURL precisely because we imagined it being useful to pass 
</I>&gt;<i> things that were not blobs to it.  It seems reasonable that passing a 
</I>&gt;<i> Foo object to createObjectURL might mint a different URL type than what 
</I>&gt;<i> we would mint for a Bar object.
</I>&gt;<i> 
</I>&gt;<i> It could also be the case that using blob: for referring to Blobs was 
</I>&gt;<i> unfortunate.  Maybe we do not really need separate URL schemes for 
</I>&gt;<i> static, fixed size things and streams.
</I>
On Mon, 15 Aug 2011, Harald Alvestrand wrote:
&gt;<i>
</I>&gt;<i> Back in ancient history (late 90s, I think), when I wrote the first 
</I>&gt;<i> version of stuff that eventually merged into RFC 4395, &quot;New URI 
</I>&gt;<i> schemes&quot;, I thought the set of operations an URI supported was pretty 
</I>&gt;<i> important.
</I>&gt;<i> 
</I>&gt;<i> In fact the text of RFC 4395 says:
</I>&gt;<i> 
</I>&gt;<i> 2.4.  Definition of Operations
</I>&gt;<i> 
</I>&gt;<i>    As part of the definition of how a URI identifies a resource, a URI
</I>&gt;<i>    scheme definition SHOULD define the applicable set of operations that
</I>&gt;<i>    may be performed on a resource using the URI as its identifier.  A
</I>&gt;<i>    model for this is HTTP; an HTTP resource can be operated on by GET,
</I>&gt;<i>    POST, PUT, and a number of other operations available through the
</I>&gt;<i>    HTTP protocol.  The URI scheme definition should describe all
</I>&gt;<i>    well-defined operations on the URI identifier, and what they are
</I>&gt;<i>    supposed to do.
</I>&gt;<i> 
</I>&gt;<i>    Some URI schemes don't fit into the &quot;information access&quot; paradigm of
</I>&gt;<i>    URIs.  For example, &quot;telnet&quot; provides location information for
</I>&gt;<i>    initiating a bi-directional data stream to a remote host; the only
</I>&gt;<i>    operation defined is to initiate the connection.  In any case, the
</I>&gt;<i>    operations appropriate for a URI scheme should be documented.
</I>&gt;<i> 
</I>&gt;<i>    Note: It is perfectly valid to say that &quot;no operation apart from GET
</I>&gt;<i>    is defined for this URI&quot;.  It is also valid to say that &quot;there's only
</I>&gt;<i>    one operation defined for this URI, and it's not very GET-like&quot;.  The
</I>&gt;<i>    important point is that what is defined on this scheme is described.
</I>&gt;<i> 
</I>&gt;<i> So if that consideration is still of concern, the next question is of 
</I>&gt;<i> course &quot;are there operations that make sense for a stream that don't 
</I>&gt;<i> make sense for (current uses of) blob:, or vice versa&quot;?
</I>&gt;<i> 
</I>&gt;<i> If &quot;blob:&quot; was intended to mean &quot;reference to internal object, hand it 
</I>&gt;<i> to APIs, the APIs will tell you if they don't like them&quot;, that 
</I>&gt;<i> consideration may not be that important.
</I>
-- 
Ian Hickson               U+1047E                )\._.,--....,'``.    fL
<A HREF="http://ln.hixie.ch/">http://ln.hixie.ch/</A>       U+263A                /,   _.. \   _\  ;`._ ,.
Things that are impossible just take longer.   `._.-(,_..'--(,_..'`-.;.'
</PRE>










































































<!--endarticle-->
<!--htdig_noindex-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="076309.html">[whatwg] Behavior when &lt;script&gt; is removed from DOM
</A></li>
	<LI>Next message: <A HREF="034038.html">[whatwg] Stat. on frequency of node insertion without children
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#34037">[ date ]</a>
              <a href="thread.html#34037">[ thread ]</a>
              <a href="subject.html#34037">[ subject ]</a>
              <a href="author.html#34037">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">More information about the whatwg
mailing list</a><br>
<!--/htdig_noindex-->
</body></html>
