<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [whatwg] Content Restrictions
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20Content%20Restrictions&In-Reply-To=%3Cop.s55n15dp1h6og4%40pancake.feldgendler.ru%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="048183.html">
   <LINK REL="Next"  HREF="048195.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[whatwg] Content Restrictions</H1>
<!--htdig_noindex-->
    <B>Alexey Feldgendler</B> 
    <A HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20Content%20Restrictions&In-Reply-To=%3Cop.s55n15dp1h6og4%40pancake.feldgendler.ru%3E"
       TITLE="[whatwg] Content Restrictions">alexey at feldgendler.ru
       </A><BR>
    <I>Thu Mar  9 08:57:31 PST 2006</I>
    <P><UL>
        <LI>Previous message: <A HREF="048183.html">[whatwg] Content Restrictions
</A></li>
        <LI>Next message: <A HREF="048195.html">[whatwg] WA1: Lists
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#48212">[ date ]</a>
              <a href="thread.html#48212">[ thread ]</a>
              <a href="subject.html#48212">[ subject ]</a>
              <a href="author.html#48212">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--/htdig_noindex-->
<!--beginarticle-->
<PRE>On Mon, 06 Mar 2006 16:48:08 +0600, Gervase Markham &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">gerv at mozilla.org</A>&gt;  
wrote:

&gt;&gt;<i> I never said that the website won't have to do HTML cleaning for
</I>&gt;&gt;<i> user-supplied content. But with HTML 5 reference parsing algorithm, such
</I>&gt;&gt;<i> cleaning is going to be much easier and straightforward: parse the text
</I>&gt;&gt;<i> into DOM (as if it was inside BODY, for example), remove or modify
</I>&gt;&gt;<i> forbidden elements, then serialize it. That way, &lt;/SANDBOX&gt; will be
</I>&gt;&gt;<i> ignored as an easy parse error because it doesn't match an opening tag
</I>&gt;&gt;<i> within the user-supplied text. An unclosed comment will be ignored, too.
</I>
&gt;<i> Er, what defines &quot;the user-supplied content&quot;? Surely it's the &lt;SANDBOX&gt;
</I>&gt;<i> tags? So how can you say &quot;A &lt;/SANDBOX&gt; inside the user-supplied content
</I>&gt;<i> will be ignored&quot;, as you don't know whether a &lt;/SANDBOX&gt; you encounter
</I>&gt;<i> is the end of the sandbox or not?
</I>&gt;<i>
</I>&gt;<i> Or are you suggesting that only one sandbox per page is allowed, and the
</I>&gt;<i> user agent should use the outermost &lt;/SANDBOX&gt; tag?
</I>
It's my fault, I just didn't make it clear enough. Here is the scenario I  
was keeping in mind.

Let's imagine a blogging website that allows anybody to create a blog  
which is available as <A HREF="http://www.example.com/blogs/username/.">http://www.example.com/blogs/username/.</A> Many such  
sites allow various user customization, so imagine this site lets the blog  
owner to supply custom HTML to display on top of the blog page. This is  
primarily used by blog authors to design stylish navigation. To make such  
navigation menus more attractive, the authors wish to use JavaScript and  
Flash, but unrestricted JavaScript would make it possible for the blog  
owner to steal visitors' session cookies.

The blog author logs in and opens some kind of customization screen:

HTML to display on top of your blog: [TEXTAREA]
[SUBMIT]

So, imagine the blog author enters into the textarea:

Welcome to my blog!&lt;/sandbox&gt;&lt;a href=&quot;#&quot;  
onclick=&quot;alert(document.cookie)&quot;&gt;Click here&lt;/a&gt;

After submission, this code is fed to the HTML cleaner. At present, HTML  
cleaners are usually complicated scripts which try to catch known quirks  
of the user agents, and still they usually have security holes found one  
after another. See for example  
<A HREF="http://cvs.livejournal.org/browse.cgi/livejournal/cgi-bin/cleanhtml.pl.">http://cvs.livejournal.org/browse.cgi/livejournal/cgi-bin/cleanhtml.pl.</A>  
With HTML 5 parsing spec, there will be one single algorithm for parsing  
HTML code with well-defined error recovery. So, the HTML cleaner at the  
server side runs the HTML 5 parser on the user-supplied text, which  
produces the following DOM:

* Welcome to my blog!
* A
     href=&quot;#&quot;
     onclick=&quot;alert(document.cookie)&quot;
   * Click here

The &lt;/sandbox&gt; tag is ignored as an easy parse error because there is no  
matching &lt;sandbox&gt; tag in the user-supplied text. After parsing, the HTML  
cleaner iterates through the tree, renaming potentially unsafe elements  
and attributes, producing the following:

* Welcome to my blog!
* A
     href=&quot;#&quot;
     safe-onclick=&quot;alert(document.cookie)&quot;
   * Click here

At the final stage, the HTML cleaner re-serializes the DOM into the  
following code, which is saved into the database:

Welcome to my blog!&lt;a href=&quot;#&quot; safe-onclick=&quot;alert(document.cookie)&quot;&gt;Click  
here&lt;/a&gt;

When the site renders the blog page, it puts the &quot;HTML for page top&quot;  
inside a sandbox:

&lt;body&gt;
&lt;sandbox&gt;
Welcome to my blog!&lt;a href=&quot;#&quot; safe-onclick=&quot;alert(document.cookie)&quot;&gt;Click  
here&lt;/a&gt;
&lt;/sandbox&gt;
...
&lt;/body&gt;

Each blog entry is probably also contained in its own sandbox. This is  
even more important on the so-called friends pages, where entries by  
different authors are displayed on the same page.

When the page is rendered in a modern user agent which supports  
sandboxing, the safe-onclick attribute is interpreted exactly the same as  
onclick. When the user clicks the link, the event handler is executed.  
Because the code is inside the sandbox, it operates on a fake document  
object, so it doesn't retrieve the cookies (I think document.cookie should  
just return an empty string). The visitor's session cookies are safe.

When the page is rendered in an older user agent which doesn't support  
sandboxing, the safe-onclick attribute is ignored because it is unknown.  
When the user clicks the link, no event handler is executed, and the  
cookies are safe again.


-- 
Opera M2 8.5 on Debian Linux 2.6.12-1-k7
* Origin: X-Man's Station [ICQ: 115226275] &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">alexey at feldgendler.ru</A>&gt;

</PRE>

<!--endarticle-->
<!--htdig_noindex-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="048183.html">[whatwg] Content Restrictions
</A></li>
	<LI>Next message: <A HREF="048195.html">[whatwg] WA1: Lists
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#48212">[ date ]</a>
              <a href="thread.html#48212">[ thread ]</a>
              <a href="subject.html#48212">[ subject ]</a>
              <a href="author.html#48212">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">More information about the whatwg
mailing list</a><br>
<!--/htdig_noindex-->
</body></html>
