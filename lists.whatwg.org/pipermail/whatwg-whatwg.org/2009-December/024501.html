<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [whatwg] Web API for speech recognition and synthesis
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20Web%20API%20for%20speech%20recognition%20and%20synthesis&In-Reply-To=%3Cf907fd300912131046v76c575cfy5b3cc0157176b576%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="024499.html">
   <LINK REL="Next"  HREF="024552.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[whatwg] Web API for speech recognition and synthesis</H1>
<!--htdig_noindex-->
    <B>Ian McGraw</B> 
    <A HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20Web%20API%20for%20speech%20recognition%20and%20synthesis&In-Reply-To=%3Cf907fd300912131046v76c575cfy5b3cc0157176b576%40mail.gmail.com%3E"
       TITLE="[whatwg] Web API for speech recognition and synthesis">imcgraw at mit.edu
       </A><BR>
    <I>Sun Dec 13 10:46:46 PST 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="024499.html">[whatwg] Web API for speech recognition and synthesis
</A></li>
        <LI>Next message: <A HREF="024552.html">[whatwg] Web API for speech recognition and synthesis
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#24501">[ date ]</a>
              <a href="thread.html#24501">[ thread ]</a>
              <a href="subject.html#24501">[ subject ]</a>
              <a href="author.html#24501">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--/htdig_noindex-->
<!--beginarticle-->
<PRE>I'm new to this list, but as a speech-scientist and web developer, I wanted
to add my 2 cents.  Personally, I believe the future of speech recognition
is in the cloud.

Here are two services which provide Javascript APIs for speech recognition
(and TTS) today:

<A HREF="http://wami.csail.mit.edu/">http://wami.csail.mit.edu/</A>
<A HREF="http://www.research.att.com/projects/SpeechMashup/index.html">http://www.research.att.com/projects/SpeechMashup/index.html</A>

Both of these are research systems, and as such they are really just
proof-of-concepts.
That said, Wami's JSONP-like implementation allows Quizlet.com to use speech
recognition today on a relatively large scale, with just a few lines of
Javascript code:

<A HREF="http://quizlet.com/voicetest/415/?scatter">http://quizlet.com/voicetest/415/?scatter</A>

Since there are a lot of Google folks on this list, I recommend you talk to
Alex Gruenstein (in your speech group) who was one of the lead developers of
WAMI while at MIT.

The major limitation we found when building the system was that we had to
develop a new audio controller for every client (Java for the desktop,
custom browsers for iPhone and Android).  It would have been much simpler if
browsers came with standard microphone capture and audio streaming
capabilities.

-Ian


On Sun, Dec 13, 2009 at 12:07 PM, Weston Ruter &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">westonruter at gmail.com</A>&gt;wrote:

&gt;<i> I blogged yesterday about this topic (including a text-to-speech demo using
</I>&gt;<i> HTML5 Audio and Google Translate's TTS service); the more relevant part for
</I>&gt;<i> this thread: &lt;<A HREF="http://weston.ruter.net/projects/google-tts/">http://weston.ruter.net/projects/google-tts/</A>&gt;
</I>&gt;<i>
</I>&gt;<i> I am really excited at the prospect of text-to-speech being made available
</I>&gt;&gt;<i> on
</I>&gt;&gt;<i> the Web! It's just too bad that fetching MP3s on an remote web service is
</I>&gt;&gt;<i> the
</I>&gt;&gt;<i> only standard way of doing so currently; modern operating systems all have
</I>&gt;&gt;<i> TTS
</I>&gt;&gt;<i> capabilities, so it's a shame that web apps and can't utilize them via
</I>&gt;&gt;<i> client-side scripting. I posted to the WHATWG mailing list about such a
</I>&gt;&gt;<i> Text-To-Speech (TTS) Web API for JavaScript, and I was directed to a
</I>&gt;&gt;<i> recent
</I>&gt;&gt;<i> thread about a Web API for speech recognition and synthesis.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> Perhaps there is some momentum building here? Having TTS available in the
</I>&gt;&gt;<i> browser would boost accessibility for the seeing-impaired and improve
</I>&gt;&gt;<i> usability
</I>&gt;&gt;<i> for people on-the-go. TTS is just another technology that has
</I>&gt;&gt;<i> traditionally been
</I>&gt;&gt;<i> relegated to desktop applications, but as the open Web advances as the
</I>&gt;&gt;<i> preferred
</I>&gt;&gt;<i> platform for application development, it is an essential service to make
</I>&gt;&gt;<i> available (as with Geolocation API, Device API, etc.). And besides, I want
</I>&gt;&gt;<i> to
</I>&gt;&gt;<i> build TTS applications and my motto is: &quot;If it can't be done on the open
</I>&gt;&gt;<i> web,
</I>&gt;&gt;<i> it's not worth doing at all&quot;!
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>&gt;<i> <A HREF="http://weston.ruter.net/projects/google-tts/">http://weston.ruter.net/projects/google-tts/</A>
</I>&gt;<i>
</I>&gt;<i> Weston
</I>&gt;<i>
</I>&gt;<i> On Fri, Dec 11, 2009 at 1:35 PM, Weston Ruter &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">westonruter at gmail.com</A>&gt;wrote:
</I>&gt;<i>
</I>&gt;&gt;<i> I was just alerted about this thread from my post &quot;Text-To-Speech (TTS)
</I>&gt;&gt;<i> Web API for JavaScript&quot; at &lt;
</I>&gt;&gt;<i> <A HREF="http://lists.whatwg.org/htdig.cgi/whatwg-whatwg.org/2009-December/024453.html">http://lists.whatwg.org/htdig.cgi/whatwg-whatwg.org/2009-December/024453.html</A>&gt;.
</I>&gt;&gt;<i> Amazing how shared ideas like these seem to arise independently at the same
</I>&gt;&gt;<i> time.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I have a use-case and an additional requirement, that the time indices be
</I>&gt;&gt;<i> made available for when each word is spoken in the TTS-generated audio:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I've been working on a web app which reads text in a web page,
</I>&gt;&gt;&gt;<i> highlighting each word as it is read. For this to be possible, a
</I>&gt;&gt;&gt;<i> Text-To-Speech API is needed which is able to:
</I>&gt;&gt;&gt;<i> (1) generate the speech audio from some text, and
</I>&gt;&gt;&gt;<i> (2) include the time indicies for when each of the words in the text is
</I>&gt;&gt;&gt;<i> spoken.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> I foresee that a TTS API should integrate closely with the HTML5 Audio
</I>&gt;&gt;<i> API. For example, invoking a call to the API could return a &quot;TTS&quot; object
</I>&gt;&gt;<i> which has an instance of Audio, whose interface could be used to navigate
</I>&gt;&gt;<i> through the TTS output. For example:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> var tts = new TextToSpeech(&quot;Hello, World!&quot;);
</I>&gt;&gt;<i> tts.audio.addEventListener(&quot;canplaythrough&quot;, function(e){
</I>&gt;&gt;<i>     //tts.indices == [{startTime:0, endTime:500, text:&quot;Hello&quot;},
</I>&gt;&gt;<i> {startTime:500, endTime:1000, text:&quot;World&quot;}]
</I>&gt;&gt;<i> }, false);
</I>&gt;&gt;<i> tts.read(); //invokes tts.audio.play
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> What would be even cooler, is if the parameter passed to the TextToSpeech
</I>&gt;&gt;<i> constructor could be an Element or TextNode, and the indices would then
</I>&gt;&gt;<i> include a DOM Range in addition to the &quot;text&quot; property. A flag could also be
</I>&gt;&gt;<i> set which would result in each of these DOM ranges to be selected when it is
</I>&gt;&gt;<i> read. For example:
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> var tts = new TextToSpeech(document.querySelector(&quot;article&quot;));
</I>&gt;&gt;<i> tts.selectRangesOnRead = true;
</I>&gt;&gt;<i> tts.audio.addEventListener(&quot;canplaythrough&quot;, function(e){
</I>&gt;&gt;<i>     /*
</I>&gt;&gt;<i>     tts.indices == [
</I>&gt;&gt;<i>         {startTime:0, endTime:500, text:&quot;Hello&quot;, range:Range},
</I>&gt;&gt;<i>         {startTime:500, endTime:1000, text:&quot;World&quot;, range:Range}
</I>&gt;&gt;<i>     ]
</I>&gt;&gt;<i>     */
</I>&gt;&gt;<i> }, false);
</I>&gt;&gt;<i> tts.read();
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> In addition to the events fired by the Audio API, more events could be
</I>&gt;&gt;<i> fired when reading TTS, such as a &quot;readrange&quot; event whose event object would
</I>&gt;&gt;<i> include the index (startTime, endTime, text, range) for the range currently
</I>&gt;&gt;<i> being spoken. Such functionality would make the ability to &quot;read along&quot; with
</I>&gt;&gt;<i> the text trivial.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> What do you think?
</I>&gt;&gt;<i> Weston
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> On Thu, Dec 3, 2009 at 4:06 AM, Bjorn Bringert &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">bringert at google.com</A>&gt;wrote:
</I>&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> On Wed, Dec 2, 2009 at 10:20 PM, Jonas Sicking &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">jonas at sicking.cc</A>&gt; wrote:
</I>&gt;&gt;&gt;<i> &gt; On Wed, Dec 2, 2009 at 11:17 AM, Bjorn Bringert &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">bringert at google.com</A>&gt;
</I>&gt;&gt;&gt;<i> wrote:
</I>&gt;&gt;&gt;<i> &gt;&gt; I agree that being able to capture and upload audio to a server would
</I>&gt;&gt;&gt;<i> &gt;&gt; be useful for a lot of applications, and it could be used to do speech
</I>&gt;&gt;&gt;<i> &gt;&gt; recognition. However, for a web app developer who just wants to
</I>&gt;&gt;&gt;<i> &gt;&gt; develop an application that uses speech input and/or output, it
</I>&gt;&gt;&gt;<i> &gt;&gt; doesn't seem very convenient, since it requires server-side
</I>&gt;&gt;&gt;<i> &gt;&gt; infrastructure that is very costly to develop and run. A
</I>&gt;&gt;&gt;<i> &gt;&gt; speech-specific API in the browser gives browser implementors the
</I>&gt;&gt;&gt;<i> &gt;&gt; option to use on-device speech services provided by the OS, or
</I>&gt;&gt;&gt;<i> &gt;&gt; server-side speech synthesis/recognition.
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> &gt; Again, it would help a lot of you could provide use cases and
</I>&gt;&gt;&gt;<i> &gt; requirements. This helps both with designing an API, as well as
</I>&gt;&gt;&gt;<i> &gt; evaluating if the use cases are common enough that a dedicated API is
</I>&gt;&gt;&gt;<i> &gt; the best solution.
</I>&gt;&gt;&gt;<i> &gt;
</I>&gt;&gt;&gt;<i> &gt; / Jonas
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> I'm mostly thinking about speech web apps for mobile devices. I think
</I>&gt;&gt;&gt;<i> that's where speech makes most sense as an input and output method,
</I>&gt;&gt;&gt;<i> because of the poor keyboards, small screens, and frequent hands/eyes
</I>&gt;&gt;&gt;<i> busy situations (e.g. while driving). Accessibility is the other big
</I>&gt;&gt;&gt;<i> reason for using speech.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Some ideas for use cases:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> - Search by speaking a query
</I>&gt;&gt;&gt;<i> - Speech-to-speech translation
</I>&gt;&gt;&gt;<i> - Voice Dialing (could open a tel: URI to actually make the call)
</I>&gt;&gt;&gt;<i> - Dialog systems (e.g. the canonical pizza ordering system)
</I>&gt;&gt;&gt;<i> - Lightweight JavaScript browser extensions (e.g. Greasemonkey /
</I>&gt;&gt;&gt;<i> Chrome extensions) for using speech with any web site, e.g, for
</I>&gt;&gt;&gt;<i> accessibility.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> Requirements:
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> - Web app developer side:
</I>&gt;&gt;&gt;<i>   - Allows both speech recognition and synthesis.
</I>&gt;&gt;&gt;<i>   - Easy to use API. Makes simple things easy and advanced things
</I>&gt;&gt;&gt;<i> possible.
</I>&gt;&gt;&gt;<i>   - Doesn't require web app developer to develop / run his own speech
</I>&gt;&gt;&gt;<i> recognition / synthesis servers.
</I>&gt;&gt;&gt;<i>   - (Natural) language-neutral API.
</I>&gt;&gt;&gt;<i>   - Allows developer-defined application specific grammars / language
</I>&gt;&gt;&gt;<i> models.
</I>&gt;&gt;&gt;<i>   - Allows multilingual applications.
</I>&gt;&gt;&gt;<i>   - Allows easy localization of speech apps.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> - Implementor side:
</I>&gt;&gt;&gt;<i>   - Easy enough to implement that it can get wide adoption in browsers.
</I>&gt;&gt;&gt;<i>   - Allows implementor to use either client-side or server-side
</I>&gt;&gt;&gt;<i> recognition and synthesis.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;&gt;<i> --
</I>&gt;&gt;&gt;<i> Bjorn Bringert
</I>&gt;&gt;&gt;<i> Google UK Limited, Registered Office: Belgrave House, 76 Buckingham
</I>&gt;&gt;&gt;<i> Palace Road, London, SW1W 9TQ
</I>&gt;&gt;&gt;<i> Registered in England Number: 3977902
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;&gt;<i>
</I>&gt;<i>
</I>-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://lists.whatwg.org/pipermail/whatwg-whatwg.org/attachments/20091213/f1660b78/attachment.htm">http://lists.whatwg.org/pipermail/whatwg-whatwg.org/attachments/20091213/f1660b78/attachment.htm</A>&gt;
</PRE>


<!--endarticle-->
<!--htdig_noindex-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="024499.html">[whatwg] Web API for speech recognition and synthesis
</A></li>
	<LI>Next message: <A HREF="024552.html">[whatwg] Web API for speech recognition and synthesis
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#24501">[ date ]</a>
              <a href="thread.html#24501">[ thread ]</a>
              <a href="subject.html#24501">[ subject ]</a>
              <a href="author.html#24501">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">More information about the whatwg
mailing list</a><br>
<!--/htdig_noindex-->
</body></html>
