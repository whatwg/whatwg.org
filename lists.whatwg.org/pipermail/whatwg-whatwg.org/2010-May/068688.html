<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [whatwg] Speech input element
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20Speech%20input%20element&In-Reply-To=%3CAANLkTin3w7CSrWO2gMzEssIl_1-VCfy2Gg9lfjJ5sICj%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="068663.html">
   <LINK REL="Next"  HREF="068689.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[whatwg] Speech input element</H1>
<!--htdig_noindex-->
    <B>Bjorn Bringert</B> 
    <A HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20Speech%20input%20element&In-Reply-To=%3CAANLkTin3w7CSrWO2gMzEssIl_1-VCfy2Gg9lfjJ5sICj%40mail.gmail.com%3E"
       TITLE="[whatwg] Speech input element">bringert at google.com
       </A><BR>
    <I>Thu May 20 05:29:16 PDT 2010</I>
    <P><UL>
        <LI>Previous message: <A HREF="068663.html">[whatwg] Speech input element
</A></li>
        <LI>Next message: <A HREF="068689.html">[whatwg] Speech input element
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#68688">[ date ]</a>
              <a href="thread.html#68688">[ thread ]</a>
              <a href="subject.html#68688">[ subject ]</a>
              <a href="author.html#68688">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--/htdig_noindex-->
<!--beginarticle-->
<PRE>On Wed, May 19, 2010 at 10:38 PM, David Singer &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">singer at apple.com</A>&gt; wrote:
&gt;<i> I am a little concerned that we are increasingly breaking down a metaphor, a 'virtual interface' without realizing what that abstraction buys us. &#160;At the moment, we have the concept of a hypothetical pointer and hypothetical keyboard, (with some abstract states, such as focus) that you can actually drive using a whole bunch of physical modalities. &#160;If we develop UIs that are specific to people actually speaking, we have 'torn the veil' of that abstract interface. &#160;What happens to people who cannot speak, for example? Or who cannot say the language needed well enough to be recognized?
</I>
It should be possible to drive &lt;input type=&quot;speech&quot;&gt; with keyboard
input, if the user agent chooses to implement that. Nothing in the API
should require the user to actually speak. I think this is a strong
argument for why &lt;input type=&quot;speech&quot;&gt; should not be replaced by a
microphone API and a separate speech recognizer, since the latter
would be very hard to make accessible. (I still think that there
should be a microphone API for applications like audio chat, but
that's a separate discussion).

-- 
Bjorn Bringert
Google UK Limited, Registered Office: Belgrave House, 76 Buckingham
Palace Road, London, SW1W 9TQ
Registered in England Number: 3977902

</PRE>

<!--endarticle-->
<!--htdig_noindex-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="068663.html">[whatwg] Speech input element
</A></li>
	<LI>Next message: <A HREF="068689.html">[whatwg] Speech input element
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#68688">[ date ]</a>
              <a href="thread.html#68688">[ thread ]</a>
              <a href="subject.html#68688">[ subject ]</a>
              <a href="author.html#68688">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">More information about the whatwg
mailing list</a><br>
<!--/htdig_noindex-->
</body></html>
