<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [whatwg] Codecs for &lt;audio&gt; and &lt;video&gt;
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20Codecs%20for%20%3Caudio%3E%20and%20%3Cvideo%3E&In-Reply-To=%3C4A54C84A.2050809%40jumis.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="063145.html">
   <LINK REL="Next"  HREF="063157.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[whatwg] Codecs for &lt;audio&gt; and &lt;video&gt;</H1>
<!--htdig_noindex-->
    <B>Charles Pritchard</B> 
    <A HREF="mailto:whatwg%40lists.whatwg.org?Subject=Re%3A%20%5Bwhatwg%5D%20Codecs%20for%20%3Caudio%3E%20and%20%3Cvideo%3E&In-Reply-To=%3C4A54C84A.2050809%40jumis.com%3E"
       TITLE="[whatwg] Codecs for &lt;audio&gt; and &lt;video&gt;">chuck at jumis.com
       </A><BR>
    <I>Wed Jul  8 09:24:42 PDT 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="063145.html">[whatwg] Codecs for &lt;audio&gt; and &lt;video&gt;
</A></li>
        <LI>Next message: <A HREF="063157.html">[whatwg] Codecs for &lt;audio&gt; and &lt;video&gt;
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#63147">[ date ]</a>
              <a href="thread.html#63147">[ thread ]</a>
              <a href="subject.html#63147">[ subject ]</a>
              <a href="author.html#63147">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--/htdig_noindex-->
<!--beginarticle-->
<PRE>On 7/8/09 2:20 AM, Philip Jagenstedt wrote:
&gt;<i> On Tue, 07 Jul 2009 22:45:41 +0200, Charles Pritchard 
</I>&gt;<i> &lt;<A HREF="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">chuck at jumis.com</A>&gt; wrote:
</I>&gt;&gt;<i> At some point, a Blob / Stream API could make things like this easier.
</I>&gt;&gt;&gt;<i> If the idea is to write a Vorbis decoder in JavaScript that would be 
</I>&gt;&gt;&gt;<i> quite cool in a way, but for vendors already implementing Vorbis it 
</I>&gt;&gt;&gt;<i> wouldn't really add anything. A pure JS-implementation of any modern 
</I>&gt;&gt;&gt;<i> audio codec would probably be a ridiculous amount of code and slow, 
</I>&gt;&gt;&gt;<i> so I doubt it would be that useful in practice.
</I>&gt;&gt;&gt;<i>
</I>&gt;&gt;<i> Well I'd like to disagree, and reiterate my prior arguments.  Vorbis 
</I>&gt;&gt;<i> decoders have been written in ActionScript and in Java.
</I>&gt;&gt;<i> They are not ridiculous, in size, nor in CPU usage. They can play 
</I>&gt;&gt;<i> audio streams, smoothly, and the file size is completely
</I>&gt;&gt;<i> tolerable. And the idea is codec neutrality, a Vorbis decoder is just 
</I>&gt;&gt;<i> one example.
</I>&gt;<i>
</I>&gt;<i> OK, I won't make any assumptions of the size/speed of such an 
</I>&gt;<i> implementation until I see one.
</I>Well,  again, there exist implementations running on Sun/Oracle's Java 
VM and the Flash VM.
These two use byte-code packaging, so the file size is under 100k, 
deflated ECMAScript
source would also weigh under 100k.

&gt;&gt;<i> Transcoding lossy data is a sub-optimal solution. Allowing for 
</I>&gt;&gt;<i> arbitrary &lt;audio&gt;
</I>&gt;&gt;<i> codecs is a worthwhile endeavor. ECMAScript can detect if playback is 
</I>&gt;&gt;<i> too slow.
</I>I want to point this out again.

While there is some struggle to define a standard codec (so we might be 
spared the burden
of so very many encoders), there is a very large supply of 
already-encoded media in the wild.

I've recently worked on a project that required a difficult to 
obtain/install codec.
Open source descriptions were available, and if it was an option, I 
certainly would have
paid to have the codec written in ECMAScript, and delivered it with the 
media files.

In that particular case, paying someone to write a decoder for one 
particular, minority codec,
would have been cheaper, and more correct, than paying for the 
transcoding of 60 gigs of low bit-rate audio.

Most media formats are lossy, making their current format, whatever the 
encumbrance, the best solution.
&gt;&gt;<i>
</I>&gt;&gt;<i> Additionally, in some cases, the programmer could work-around broken 
</I>&gt;&gt;<i> codec implementations.
</I>&gt;&gt;<i> It's forward-looking, it allows real backward compatibility and 
</I>&gt;&gt;<i> interoperability across browsers.
</I>&gt;&gt;<i>
</I>&gt;&gt;<i> &lt;canvas&gt; allows for arbitrary, programmable video, &lt;audio&gt; should allow
</I>&gt;&gt;<i> for programmable audio. Then, we can be codec neutral in our media 
</I>&gt;&gt;<i> elements.
</I>&gt;<i>
</I>&gt;<i> While stressing that I don't think this should go into the spec until 
</I>&gt;<i> there's a proof-of-concept implementation that does useful stuff, is 
</I>&gt;<i> the idea to set audio.src=new MySynthesizer() and play()? 
</I>&gt;<i> (MySynthesizer would need to implement some standard interface.) You 
</I>&gt;<i> also have the question of push vs pull, i.e. does the audio source 
</I>&gt;<i> request data from the synthesizer when needed or does the synthesizer 
</I>&gt;<i> need to run a loop pushing audio data?
</I>&gt;<i>
</I>Well we really need to define what useful stuff is, you know, to set 
that bar.

There are two use cases that I think are important: a codec 
implementation (let's use Vorbis),
and an accessibility implementation, working with a &lt;canvas&gt; element.

I don't know what would qualify for accessibility. A topographical map, 
which makes a lower or higher
pitched hum, based on elevation (surrounding the pointer), is an example.

On that same line of thinking, a hum of varying intensity signaling 
proximity to a clickable element,
(we're still talking about &lt;canvas&gt;) might be useful.  If there is no 
sound in the right-channel,
there are no elements to be clicked on, to the right of the pointer. If 
it is a low-sound, then the
element is rather far away.

Site developers still need to put in the work. With a buffered audio 
API, they'll at least
have the option to do so.

Can we come to an agreement as to what would constitute a reasonable 
proof of concept?
This is meant to allow &lt;canvas&gt; to be more accessible to the visually 
impaired.

Obviously, &lt;audio src&gt; tags could be used in many cases with &lt;canvas&gt;, 
so our test-case
should be one where &lt;audio src&gt; would be insufficient.

Both of these use cases can be accomplished with a raw audio buffer.
They do not need native channel mixing, nor toDataURL support.

In the long term, I think those two options would be nice, but in the 
short term, would just cause delays in adoption.
As Robert has said, there are &quot;much more important things to work on&quot;
( <A HREF="https://bugzilla.mozilla.org/show_bug.cgi?id=490705">https://bugzilla.mozilla.org/show_bug.cgi?id=490705</A> ).


I think at this point, the model should play buffered bytes as they are 
made available (if the buffer has anything, start playing it).

I believe the &quot;buffered&quot; attribute can be used by the ECMAScript loop to 
detect
how much data is buffered, and whether it should continue decoding or 
take other actions.

The buffered audio API should be handled by the media API in a way 
similar to streaming Web radio.

There should be an origin-clean flag, for future use. One might 
theoretically
add audio into a currently playing stream. (regardless of toDataURL 
support).


Does this sound reasonable? What I'm requesting is an append-only raw 
audio buffer, and an origin-clean flag (similar to &lt;canvas&gt;)
to be added to the &lt;audio&gt; tag, if not the Media element interface, for 
future use. The audio buffer plays immediately,
if any data is available in it.

In v2, we would discuss Vlad's getAudioSampleData proposal, native 
channel mixing (mix two audio streams, for whatever reason),
and other effects that allow the more complex &quot;audio editor&quot; use case. 
For now, let's just consider an &quot;audio player&quot;
to support arbitrary audio codecs and address accessibility for the 
visually impaired.

We need Audio.appendBuffer, Audio.createBufferArray
and an AudioBufferArray interface of some sort, and I think it's good to go.

The naming and arguments still need to be worked out.

I'd enthusiastically support such an interface in Java, Flash and 
.Net/Active X plugins.
For the legacy/IE crowd.

-Charles

</PRE>

<!--endarticle-->
<!--htdig_noindex-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="063145.html">[whatwg] Codecs for &lt;audio&gt; and &lt;video&gt;
</A></li>
	<LI>Next message: <A HREF="063157.html">[whatwg] Codecs for &lt;audio&gt; and &lt;video&gt;
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#63147">[ date ]</a>
              <a href="thread.html#63147">[ thread ]</a>
              <a href="subject.html#63147">[ subject ]</a>
              <a href="author.html#63147">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.whatwg.org/listinfo.cgi/whatwg-whatwg.org">More information about the whatwg
mailing list</a><br>
<!--/htdig_noindex-->
</body></html>
